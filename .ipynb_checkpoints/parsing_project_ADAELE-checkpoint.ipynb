{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sys import stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Marvel dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Parse the Marvel characters\n",
    "\n",
    "**We go on the page where all the `Earth-616` Marvel characters are. We start with the first page and we make a list of all the URLs for the different characters.**\n",
    "\n",
    "**Collect the first URLs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://marvel.fandom.com/wiki/Category:Earth-616_Characters'\n",
    "r = requests.get(URL)\n",
    "page_body = r.text\n",
    "soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "# Collect url of each character in the web page\n",
    "publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "list_url = []\n",
    "\n",
    "for p in publications_wrappers:\n",
    "    for a in p.find_all('a', href=True):\n",
    "        list_url.append(a['href'])\n",
    "        \n",
    "# Remove duplicate\n",
    "my_set = set(list_url)\n",
    "good_list_url = list(my_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We then use these URLs to collect the needed characterstics for each Marvel character.**\n",
    "\n",
    "**Parse the first page:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "new_columns = ['URL', 'Real Name', 'Identity', 'Current Alias', 'Citizenship',\n",
    "                'Marital Status', 'Occupation',\n",
    "                'Education', 'Gender', 'Height', 'Weight',\n",
    "                'Eyes','Hair',\n",
    "                'Place of Birth']\n",
    "\n",
    "personnage_pd = pd.DataFrame(columns=new_columns)\n",
    "idx = 0\n",
    "dict_geant={}\n",
    "\n",
    "for pers in good_list_url:\n",
    "    # Get URL and use html parser\n",
    "    URL_char = 'https://marvel.fandom.com' + pers\n",
    "    URL_char = URL_char.replace(\"'\",\"\")\n",
    "    r_char = requests.get(URL_char)\n",
    "    page_body_char = r_char.text\n",
    "    soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "    \n",
    "    # Initialize variables\n",
    "    url, name, identity, current_alias, citizenship, marital_status, occupation = '','','','','','',''\n",
    "    education, gender, height, weight, eyes, hair, place_of_birth = '','','','','','',''\n",
    "    personnage =[]\n",
    "    \n",
    "    # Parsing\n",
    "    url = pers\n",
    "    side_tab = soup_char.find_all('div', class_='conjoined-infoboxes')\n",
    "    for p in side_tab:\n",
    "        for pp in p.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
    "            for ppp in pp.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "                if(ppp.text[1:]==\"Real Name\"):\n",
    "                    name = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[1:]==\"Identity\"):\n",
    "                    identity = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[1:]==\"Current Alias\"):\n",
    "                    current_alias = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[1:]==\"Citizenship\"):\n",
    "                    citizenship = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[1:]==\"Marital Status\"):\n",
    "                    marital_status = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[1:]==\"Occupation\"):\n",
    "                    occupation = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[1:]==\"Education\"):\n",
    "                    education = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[1:]==\"Gender\"):\n",
    "                    gender = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[1:]==\"Height\"):\n",
    "                    height = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[1:]==\"Weight\"):\n",
    "                    weight = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[1:]==\"Eyes\"):\n",
    "                    eyes = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[1:]==\"Hair\"):\n",
    "                    hair = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[1:]==\"Place of Birth\"):\n",
    "                    place_of_birth = pp.find('div', class_='pi-data-value pi-font').text\n",
    "\n",
    "    characteristics_pd = pd.DataFrame([[url, name[1:], identity, current_alias, citizenship, marital_status, occupation,\n",
    "                         education, gender, height, weight, eyes, hair, place_of_birth]], columns = new_columns)\n",
    "    personnage_pd = personnage_pd.append(characteristics_pd,ignore_index=True)\n",
    "    \n",
    "nextpage = soup.find('link', {\"rel\" : \"next\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After parsing the first page, we can get the links for the next pages, parsing them one by one. After that, we use the collected links to access each character's webpage exactly as we did for the first page. We then follow by storing the characterstics we need for our analysis.**\n",
    "\n",
    "**Parse the remaining pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(len(nextpage['href'])):\n",
    "    urlnext_page = nextpage['href']\n",
    "    r = requests.get(urlnext_page)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "    # Collect URL of each character in the web page\n",
    "    publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "    list_url = []\n",
    "    for p in publications_wrappers:\n",
    "        for a in p.find_all('a', href=True):\n",
    "            list_url.append(a['href'])\n",
    "            \n",
    "    # Remove duplicate\n",
    "    my_set = set(list_url)\n",
    "    good_list_url = list(my_set)\n",
    "    \n",
    "    idx = 0\n",
    "    dict_geant = {}\n",
    "    \n",
    "    for pers in good_list_url:\n",
    "        # Get URL and use html parser\n",
    "        URL_char = 'https://marvel.fandom.com' + pers\n",
    "        URL_char = URL_char.replace(\"'\",\"\")\n",
    "        r_char = requests.get(URL_char)\n",
    "        page_body_char = r_char.text\n",
    "        soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "        \n",
    "        # Initialize variables\n",
    "        personnage =[]\n",
    "        \n",
    "        # Parsing\n",
    "        side_tab = soup_char.find_all('div', class_='conjoined-infoboxes')\n",
    "        url = pers\n",
    "        for p in side_tab:\n",
    "            for pp in p.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
    "                for ppp in pp.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "                    if(ppp.text[1:]==\"Real Name\"):\n",
    "                        name = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[1:]==\"Identity\"):\n",
    "                        identity = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[1:]==\"Current Alias\"):\n",
    "                        current_alias = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[1:]==\"Citizenship\"):\n",
    "                        citizenship = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[1:]==\"Marital Status\"):\n",
    "                        marital_status = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[1:]==\"Occupation\"):\n",
    "                        occupation = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[1:]==\"Education\"):\n",
    "                        education = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[1:]==\"Gender\"):\n",
    "                        gender = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[1:]==\"Height\"):\n",
    "                        height = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[1:]==\"Weight\"):\n",
    "                        weight = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[1:]==\"Eyes\"):\n",
    "                        eyes = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[1:]==\"Hair\"):\n",
    "                        hair = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                    if(ppp.text[1:]==\"Place of Birth\"):\n",
    "                        place_of_birth = pp.find('div', class_='pi-data-value pi-font').text\n",
    "\n",
    "        characteristics_pd = pd.DataFrame([[url, name[1:], identity, current_alias, citizenship, marital_status, occupation,\n",
    "                          education, gender, height, weight, eyes, hair, place_of_birth]], columns = new_columns)\n",
    "        personnage_pd = personnage_pd.append(characteristics_pd,ignore_index=True)\n",
    "        \n",
    "        nextpage = soup.find('link', {\"rel\" : \"next\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since the parsing takes a relatively high amount of time, it's better to save it to pickle.**\n",
    "\n",
    "**Save file to pickle:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(personnage_pd, open('data_pickle/characters_marvel.txt','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open the saved file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Real Name</th>\n",
       "      <th>Identity</th>\n",
       "      <th>Current Alias</th>\n",
       "      <th>Citizenship</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Hair</th>\n",
       "      <th>Place of Birth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Aaron_the_Aakon_(Earth-616)</td>\n",
       "      <td>Aaron</td>\n",
       "      <td>Secret Identity</td>\n",
       "      <td></td>\n",
       "      <td>Aakon</td>\n",
       "      <td>Single</td>\n",
       "      <td>Slave trader</td>\n",
       "      <td></td>\n",
       "      <td>Male</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Brown</td>\n",
       "      <td>Black</td>\n",
       "      <td>Planet Oorga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>/wiki/2-D_(Earth-616)</td>\n",
       "      <td>Darell (full name unrevealed)[1]</td>\n",
       "      <td>Secret Identity</td>\n",
       "      <td>2-D</td>\n",
       "      <td>American</td>\n",
       "      <td>Single</td>\n",
       "      <td>Adventurer</td>\n",
       "      <td></td>\n",
       "      <td>Male</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Brown</td>\n",
       "      <td>Brown</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>/wiki/Abraham_Erskine_(Earth-616)</td>\n",
       "      <td>Abraham Erskine[1]</td>\n",
       "      <td>Known to Authorities Identity</td>\n",
       "      <td>Dr. Joseph Reinstein</td>\n",
       "      <td>German, American</td>\n",
       "      <td>Married</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>Advanced College Degree</td>\n",
       "      <td>Male</td>\n",
       "      <td>5' 6\" (1.68 m)</td>\n",
       "      <td>160 lbs (73 kg)</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Black (graying)</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>/wiki/11-Ball_(Earth-616)</td>\n",
       "      <td>nknown</td>\n",
       "      <td>Secret Identity</td>\n",
       "      <td>11-Ball</td>\n",
       "      <td>American</td>\n",
       "      <td>Single</td>\n",
       "      <td>Professional criminal; former henchman</td>\n",
       "      <td></td>\n",
       "      <td>Male</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>/wiki/Abraham_(Earth-616)</td>\n",
       "      <td>Abraham</td>\n",
       "      <td>No Dual Identity</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Married</td>\n",
       "      <td>Prophet</td>\n",
       "      <td></td>\n",
       "      <td>Male</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Black</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28004</td>\n",
       "      <td>/wiki/Zyziwc_Tiel_(Earth-616)</td>\n",
       "      <td>Zyziwc Tiel</td>\n",
       "      <td></td>\n",
       "      <td>Nova</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Corpsman</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28005</td>\n",
       "      <td>/wiki/Zzzax_(Earth-616)</td>\n",
       "      <td>Zzzax</td>\n",
       "      <td>No Dual Identity</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Single</td>\n",
       "      <td>Purveyor of destruction</td>\n",
       "      <td></td>\n",
       "      <td>Agender</td>\n",
       "      <td>40' 0\" (12.19 m) (Maximum), Variable</td>\n",
       "      <td>0 lbs (0 kg)</td>\n",
       "      <td>None</td>\n",
       "      <td>No Hair</td>\n",
       "      <td>Consolidated Edison Power Plant, New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28006</td>\n",
       "      <td>/wiki/%C3%89riu_(Earth-616)</td>\n",
       "      <td>Ériu[1]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Female</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28007</td>\n",
       "      <td>/wiki/Zyro_(Earth-616)</td>\n",
       "      <td>Zyro</td>\n",
       "      <td>No Dual Identity</td>\n",
       "      <td></td>\n",
       "      <td>Kree Empire</td>\n",
       "      <td></td>\n",
       "      <td>Starship chief-tech</td>\n",
       "      <td></td>\n",
       "      <td>Male</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>No Hair</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28008</td>\n",
       "      <td>/wiki/Zzutak_(Earth-616)</td>\n",
       "      <td>Zzutak</td>\n",
       "      <td>No Dual Identity</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Servent</td>\n",
       "      <td></td>\n",
       "      <td>Male</td>\n",
       "      <td>32' 28\" (10.46 m)</td>\n",
       "      <td>60000 lbs (27216 kg)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28009 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     URL                         Real Name  \\\n",
       "0      /wiki/Aaron_the_Aakon_(Earth-616)                             Aaron   \n",
       "1                  /wiki/2-D_(Earth-616)  Darell (full name unrevealed)[1]   \n",
       "2      /wiki/Abraham_Erskine_(Earth-616)                Abraham Erskine[1]   \n",
       "3              /wiki/11-Ball_(Earth-616)                            nknown   \n",
       "4              /wiki/Abraham_(Earth-616)                           Abraham   \n",
       "...                                  ...                               ...   \n",
       "28004      /wiki/Zyziwc_Tiel_(Earth-616)                       Zyziwc Tiel   \n",
       "28005            /wiki/Zzzax_(Earth-616)                             Zzzax   \n",
       "28006        /wiki/%C3%89riu_(Earth-616)                           Ériu[1]   \n",
       "28007             /wiki/Zyro_(Earth-616)                              Zyro   \n",
       "28008           /wiki/Zzutak_(Earth-616)                            Zzutak   \n",
       "\n",
       "                            Identity         Current Alias       Citizenship  \\\n",
       "0                    Secret Identity                                   Aakon   \n",
       "1                    Secret Identity                   2-D          American   \n",
       "2      Known to Authorities Identity  Dr. Joseph Reinstein  German, American   \n",
       "3                    Secret Identity               11-Ball          American   \n",
       "4                   No Dual Identity                                           \n",
       "...                              ...                   ...               ...   \n",
       "28004                                                 Nova                     \n",
       "28005               No Dual Identity                                           \n",
       "28006                                                                          \n",
       "28007               No Dual Identity                             Kree Empire   \n",
       "28008               No Dual Identity                                           \n",
       "\n",
       "      Marital Status                              Occupation  \\\n",
       "0             Single                            Slave trader   \n",
       "1             Single                              Adventurer   \n",
       "2            Married                               Scientist   \n",
       "3             Single  Professional criminal; former henchman   \n",
       "4            Married                                 Prophet   \n",
       "...              ...                                     ...   \n",
       "28004                                               Corpsman   \n",
       "28005         Single                 Purveyor of destruction   \n",
       "28006                                                          \n",
       "28007                                    Starship chief-tech   \n",
       "28008                                                Servent   \n",
       "\n",
       "                     Education   Gender                                Height  \\\n",
       "0                                  Male                                         \n",
       "1                                  Male                                         \n",
       "2      Advanced College Degree     Male                        5' 6\" (1.68 m)   \n",
       "3                                  Male                                         \n",
       "4                                  Male                                         \n",
       "...                        ...      ...                                   ...   \n",
       "28004                                                                           \n",
       "28005                           Agender  40' 0\" (12.19 m) (Maximum), Variable   \n",
       "28006                            Female                                         \n",
       "28007                              Male                                         \n",
       "28008                              Male                     32' 28\" (10.46 m)   \n",
       "\n",
       "                     Weight   Eyes             Hair  \\\n",
       "0                            Brown            Black   \n",
       "1                            Brown            Brown   \n",
       "2           160 lbs (73 kg)  Brown  Black (graying)   \n",
       "3                                                     \n",
       "4                                             Black   \n",
       "...                     ...    ...              ...   \n",
       "28004                                                 \n",
       "28005          0 lbs (0 kg)   None          No Hair   \n",
       "28006                                                 \n",
       "28007                                       No Hair   \n",
       "28008  60000 lbs (27216 kg)                           \n",
       "\n",
       "                                  Place of Birth  \n",
       "0                                   Planet Oorga  \n",
       "1                                                 \n",
       "2                                        Germany  \n",
       "3                                                 \n",
       "4                                                 \n",
       "...                                          ...  \n",
       "28004                                             \n",
       "28005  Consolidated Edison Power Plant, New York  \n",
       "28006                                             \n",
       "28007                                             \n",
       "28008                                             \n",
       "\n",
       "[28009 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data_pickle/characters_marvel.txt', 'rb') as f:\n",
    "    characters_marvel = pickle.load(f)\n",
    "\n",
    "characters_marvel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parse the Marvel comics\n",
    "\n",
    "**Similarly to the Marvel characters, we want to first collect the comics' URLs to get to a specific comic's page. We start by the first page.**\n",
    "\n",
    "**Collect the first URLs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get URL and use html parser\n",
    "URL = 'https://marvel.fandom.com/wiki/Category:Comics'\n",
    "r = requests.get(URL)\n",
    "page_body = r.text\n",
    "soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "# Collect URL of each character in the web page\n",
    "publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "\n",
    "# Initialization\n",
    "i = 0\n",
    "list_url = []\n",
    "\n",
    "for p in publications_wrappers:\n",
    "    # Don't take into account the first links which are \"Categories\"\n",
    "    if(i>=26):\n",
    "        for a in p.find_all('a', href=True):\n",
    "            list_url.append(a['href'])\n",
    "    i +=1\n",
    "    \n",
    "list_url[:20]\n",
    "\n",
    "# Remove duplicate\n",
    "my_set = set(list_url)\n",
    "good_list_url = list(my_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After getting the URLs, we parse all the information we need from each page. Let's note that a comic can have more than one story that are separate. We will name them `subcomic`. They will be treated as different comics since the appearing characters and the writers are often different.**\n",
    "\n",
    "**Parse the first page:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comics_columns = ['URL', 'Good characters', 'Bad characters', \n",
    "                  'Neutral characters', 'Editor-in-chief', 'Editor-in-chief URL', \n",
    "                  'Writer', 'Writer URL', 'Publication date', 'Subcomic']\n",
    "\n",
    "comics_pd = pd.DataFrame(columns=comics_columns)\n",
    "\n",
    "\n",
    "# Parse the first page\n",
    "for comics in good_list_url:\n",
    "    # Get URL and use html parser\n",
    "    URL_char = 'https://marvel.fandom.com' + comics\n",
    "    URL_char = URL_char.replace(\"'\",\"\")\n",
    "    r_char = requests.get(URL_char)\n",
    "    page_body_char = r_char.text\n",
    "    soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "    \n",
    "    # Initialization\n",
    "    good, bad, neutral = '','',''\n",
    "    editor, writer, publication = '','',''\n",
    "    editorURL, writerURL, subcomic = '','',''\n",
    "    \n",
    "    URL2 = URL_char.replace('https://marvel.fandom.com','')\n",
    "\n",
    "    # Boolean for the first case\n",
    "    first = 1\n",
    "\n",
    "    # Parse characters appearances\n",
    "    appearances = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "\n",
    "    for p in appearances:\n",
    "        for pp in p.find_all('p'):\n",
    "            # If a comic is split in sub-comics\n",
    "            span = pp.find_previous('span', class_='mw-headline')\n",
    "            \n",
    "            if span and (\"Appearing\" in span.text):\n",
    "                if (not first and subcomic!=span.text \\\n",
    "                and (good or bad or neutral)):\n",
    "                    # Take only the title of the subcomic\n",
    "                    subcomic = subcomic[14:-1]\n",
    "                    appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                        editor, editorURL,\n",
    "                                        writer, writerURL,\n",
    "                                        publication, subcomic]], columns = comics_columns)\n",
    "                    comics_pd = comics_pd.append(appearances_pd)\n",
    "\n",
    "                    good, bad, neutral = '','',''\n",
    "                    editor, writer, publication = '','',''\n",
    "                    editorURL, writerURL = '',''\n",
    "                    subcomic = span.text\n",
    "\n",
    "                else:\n",
    "                    first = 0\n",
    "                    subcomic = span.text\n",
    "\n",
    "            if \"Featured Characters:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    good = good + ', ' + a['href']\n",
    "\n",
    "            if \"Supporting Characters:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    good = good + ', ' + a['href']\n",
    "\n",
    "            if \"Antagonists:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    bad = bad + ', ' + a['href']\n",
    "\n",
    "            if \"Other Characters:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    neutral = neutral + ', ' + a['href']\n",
    "\n",
    "    # Each subcomic is written in the format '\"Appearing in *subcomic*\"'\n",
    "    # We keep only the subcomic name\n",
    "    subcomic = subcomic[14:-1]\n",
    "    appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                    editor, editorURL,\n",
    "                                    writer, writerURL,\n",
    "                                    publication, subcomic]], columns = comics_columns)\n",
    "\n",
    "    comics_pd = comics_pd.append(appearances_pd)\n",
    "\n",
    "    subcomic = ''\n",
    "\n",
    "\n",
    "    # Parse comic characteristics\n",
    "    side_tab = soup_char.find_all('div', class_='infobox')\n",
    "\n",
    "    for p in side_tab:\n",
    "        # Publication date\n",
    "        tab = p.find_next('table', style='width:100%; text-align: center;')\n",
    "        if(tab):\n",
    "            tr = tab.find_next('tr', style='font-size:12px;')\n",
    "            td = tab.find_next('td', style='font-size:12px;')\n",
    "        if (tr):\n",
    "            publication = tr.find_next('td').text\n",
    "        if (td):\n",
    "            publication = td.text\n",
    "        if (not tr and not td):\n",
    "            publication = ''\n",
    "\n",
    "        # Editors\n",
    "        for pp in p.find_all('div', style='width:100px;float:left;text-align:left;'):\n",
    "            if ('Editor-in-Chief' in pp.text):\n",
    "                ppp = pp.find_next('div', style='width:190px;float:left;text-align:right;')\n",
    "                for a in ppp.find_all('a'):\n",
    "                    editor = editor + ', ' + a.text\n",
    "                for a in ppp.find_all('a', href=True):\n",
    "                    editorURL = editorURL + ', ' + a['href']\n",
    "                    \n",
    "        if(p.find_next('div', style='width:88%; text-align:left;')):\n",
    "            sub = p.find_next('div', style='width:88%; text-align:left;')\n",
    "\n",
    "        # Writers\n",
    "        for pp in p.find_all('div', style='width:100px;float:left;text-align:left;'):\n",
    "            if (pp.find_previous('div', style='width:88%; text-align:left;')):\n",
    "                sub = pp.find_previous('div', style='width:88%; text-align:left;')\n",
    "            if sub:\n",
    "                adiv = sub.find_next('div', style='width:100px;float:left;text-align:left;')\n",
    "\n",
    "            if (adiv and 'Writer' in adiv.text and 'Writer' in pp.text):\n",
    "                if (sub):\n",
    "                    subcomic = sub.text[1:-1]\n",
    "\n",
    "                ppp = pp.find_next('div', style='width:190px;float:left;text-align:right;')\n",
    "                for a in ppp.find_all('a'):\n",
    "                    writer = writer + ', ' + a.text\n",
    "                for a in ppp.find_all('a', href=True):\n",
    "                    writerURL = writerURL + ', ' + a['href']\n",
    "\n",
    "                comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer'] = writer\n",
    "                comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer URL'] = writerURL\n",
    "\n",
    "                if (sub):\n",
    "                    sub = sub.find_next('div', style='width:88%; text-align:left;')\n",
    "                    writer, writerURL = '',''\n",
    "\n",
    "            elif (sub): \n",
    "                sub = sub.find_next('div', style='width:88%; text-align:left;')\n",
    "                writer, writerURL = '',''    \n",
    "    \n",
    "    comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief'] = editor\n",
    "    comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief URL'] = editorURL\n",
    "    comics_pd.loc[comics_pd.URL == URL2,'Publication date'] = publication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Again, we use the next link to parse the remaining pages. The schema is exactly the same as the previous one.**\n",
    "\n",
    "**Parse the remaining pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all the other pages\n",
    "nextpage = soup.find('link', {\"rel\" : \"next\"})\n",
    "\n",
    "i = 0\n",
    "tot_page = 49759/200\n",
    "\n",
    "while(len(nextpage['href'])):\n",
    "    # Print a loading bar\n",
    "    i += 1\n",
    "    printed= i/tot_page*100\n",
    "    stdout.write(\"\\r%f %%\" % printed)\n",
    "    stdout.flush()\n",
    "    urlnext_page = nextpage['href']\n",
    "    r = requests.get(urlnext_page)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "    # Collect url of each character in the web page\n",
    "    publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "    list_url = []\n",
    "    for p in publications_wrappers:\n",
    "        for a in p.find_all('a', href=True):\n",
    "            list_url.append(a['href'])\n",
    "            \n",
    "    # Remove duplicates\n",
    "    my_set = set(list_url)\n",
    "    good_list_url = list(my_set)\n",
    "    \n",
    "    for comics in good_list_url:\n",
    "        # Get URL and use html parser\n",
    "        URL_char = 'https://marvel.fandom.com' + comics\n",
    "        URL_char = URL_char.replace(\"'\",\"\")\n",
    "        r_char = requests.get(URL_char)\n",
    "        page_body_char = r_char.text\n",
    "        soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "\n",
    "        # Initialization\n",
    "        good, bad, neutral = '','',''\n",
    "        editor, writer, publication = '','',''\n",
    "        editorURL, writerURL, subcomic = '','',''\n",
    "\n",
    "        URL2 = URL_char.replace('https://marvel.fandom.com','')\n",
    "\n",
    "        # Boolean for the first case\n",
    "        first = 1\n",
    "\n",
    "        # Parse characters appearances\n",
    "        appearances = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "\n",
    "        for p in appearances:\n",
    "            for pp in p.find_all('p'):\n",
    "                # If a comic is split in sub-comics\n",
    "                span = pp.find_previous('span', class_='mw-headline')\n",
    "                \n",
    "                if span and (\"Appearing\" in span.text):\n",
    "                    if (not first and subcomic!=span.text \\\n",
    "                    and (good or bad or neutral)):\n",
    "                        # Take only the title of the subcomic\n",
    "                        subcomic = subcomic[14:-1]\n",
    "                        appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                            editor, editorURL,\n",
    "                                            writer, writerURL,\n",
    "                                            publication, subcomic]], columns = comics_columns)\n",
    "                        comics_pd = comics_pd.append(appearances_pd)\n",
    "\n",
    "                        good, bad, neutral = '','',''\n",
    "                        editor, writer, publication = '','',''\n",
    "                        editorURL, writerURL = '',''\n",
    "                        subcomic = span.text\n",
    "\n",
    "                    else:\n",
    "                        first = 0\n",
    "                        subcomic = span.text\n",
    "\n",
    "                if \"Featured Characters:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        good = good + ', ' + a['href']\n",
    "\n",
    "                if \"Supporting Characters:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        good = good + ', ' + a['href']\n",
    "\n",
    "                if \"Antagonists:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        bad = bad + ', ' + a['href']\n",
    "\n",
    "                if \"Other Characters:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        neutral = neutral + ', ' + a['href']\n",
    "\n",
    "        # Each subcomic is written in the format '\"Appearing in *subcomic*\"'\n",
    "        # We keep only the subcomic name\n",
    "        subcomic = subcomic[14:-1]\n",
    "        appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                        editor, editorURL,\n",
    "                                        writer, writerURL,\n",
    "                                        publication, subcomic]], columns = comics_columns)\n",
    "\n",
    "        comics_pd = comics_pd.append(appearances_pd)\n",
    "\n",
    "        subcomic = ''\n",
    "\n",
    "\n",
    "        # Parse comic characteristics\n",
    "        side_tab = soup_char.find_all('div', class_='infobox')\n",
    "\n",
    "        for p in side_tab:\n",
    "            # Publication date\n",
    "            if(p.find_next('div', style='width:88%; text-align:left;')):\n",
    "                tab = p.find_next('table', style='width:100%; text-align: center;')\n",
    "            if(tab):\n",
    "                tr = tab.find_next('tr', style='font-size:12px;')\n",
    "                td = tab.find_next('td', style='font-size:12px;')\n",
    "            if (tr):\n",
    "                publication = tr.find_next('td').text\n",
    "            if (td):\n",
    "                publication = td.text\n",
    "            if (not tr and not td):\n",
    "                publication = ''\n",
    "\n",
    "            # Editors\n",
    "            for pp in p.find_all('div', style='width:100px;float:left;text-align:left;'):\n",
    "                if ('Editor-in-Chief' in pp.text):\n",
    "                    ppp = pp.find_next('div', style='width:190px;float:left;text-align:right;')\n",
    "                    for a in ppp.find_all('a'):\n",
    "                        editor = editor + ', ' + a.text\n",
    "                    for a in ppp.find_all('a', href=True):\n",
    "                        editorURL = editorURL + ', ' + a['href']\n",
    "\n",
    "            sub = p.find_next('div', style='width:88%; text-align:left;')\n",
    "\n",
    "            # Writers\n",
    "            for pp in p.find_all('div', style='width:100px;float:left;text-align:left;'):\n",
    "                if (pp.find_previous('div', style='width:88%; text-align:left;')):\n",
    "                    sub = pp.find_previous('div', style='width:88%; text-align:left;')\n",
    "                if sub:\n",
    "                    adiv = sub.find_next('div', style='width:100px;float:left;text-align:left;')\n",
    "\n",
    "                if (adiv and 'Writer' in adiv.text and 'Writer' in pp.text):\n",
    "                    if (sub):\n",
    "                        subcomic = sub.text[1:-1]\n",
    "\n",
    "                    ppp = pp.find_next('div', style='width:190px;float:left;text-align:right;')\n",
    "                    for a in ppp.find_all('a'):\n",
    "                        writer = writer + ', ' + a.text\n",
    "                    for a in ppp.find_all('a', href=True):\n",
    "                        writerURL = writerURL + ', ' + a['href']\n",
    "\n",
    "                    comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer'] = writer\n",
    "                    comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer URL'] = writerURL\n",
    "\n",
    "                    if (sub):\n",
    "                        sub = sub.find_next('div', style='width:88%; text-align:left;')\n",
    "                        writer, writerURL = '',''\n",
    "\n",
    "                elif (sub): \n",
    "                    sub = sub.find_next('div', style='width:88%; text-align:left;')\n",
    "                    writer, writerURL = '',''    \n",
    "\n",
    "        comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief'] = editor\n",
    "        comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief URL'] = editorURL\n",
    "        comics_pd.loc[comics_pd.URL == URL2,'Publication date'] = publication\n",
    "    nextpage = soup.find('link', {\"rel\" : \"next\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save file to pickle:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(comics_pd, open('data_pickle/comics_marvel.txt','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open the saved file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Good characters</th>\n",
       "      <th>Bad characters</th>\n",
       "      <th>Neutral characters</th>\n",
       "      <th>Editor-in-chief</th>\n",
       "      <th>Editor-in-chief URL</th>\n",
       "      <th>Writer</th>\n",
       "      <th>Writer URL</th>\n",
       "      <th>Publication date</th>\n",
       "      <th>Subcomic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Marvel_Mystery_Comics_Vol_1_NN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Joe Caramagna</td>\n",
       "      <td>, /wiki/Joe_Caramagna</td>\n",
       "      <td>January, 1943</td>\n",
       "      <td>st stor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Comedy_Comics_Vol_1_12</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Stan Lee</td>\n",
       "      <td>, /wiki/Stan_Lee</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>December, 1942</td>\n",
       "      <td>Morphy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Marvel_Mystery_Comics_Vol_1_7</td>\n",
       "      <td>, /wiki/Human_Torch_(Android)_(Earth-616), /wi...</td>\n",
       "      <td>, /wiki/Roglo_(Earth-616), #cite_note-Only_App...</td>\n",
       "      <td>, /wiki/New_York_City_Police_Department_(Earth...</td>\n",
       "      <td>, Joe Simon</td>\n",
       "      <td>, /wiki/Joe_Simon</td>\n",
       "      <td>, Stan Lee, Larry Lieber</td>\n",
       "      <td>, /wiki/Stan_Lee, /wiki/Larry_Lieber</td>\n",
       "      <td>May, 1940</td>\n",
       "      <td>The Human Torch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Marvel_Mystery_Comics_Vol_1_7</td>\n",
       "      <td>, /wiki/Thomas_Halloway_(Earth-616), /wiki/Bet...</td>\n",
       "      <td>, /wiki/Emma_Martin_(Earth-616)</td>\n",
       "      <td>, /wiki/Henry_Martin_(Earth-616)</td>\n",
       "      <td>, Joe Simon</td>\n",
       "      <td>, /wiki/Joe_Simon</td>\n",
       "      <td>, Paul Gustavson, Ray Gill</td>\n",
       "      <td>, /wiki/Paul_Gustavson, /wiki/Ray_Gill</td>\n",
       "      <td>May, 1940</td>\n",
       "      <td>The Angel: Master of Men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Marvel_Mystery_Comics_Vol_1_7</td>\n",
       "      <td>, /wiki/Namor_McKenzie_(Earth-616), /wiki/Thak...</td>\n",
       "      <td></td>\n",
       "      <td>, /wiki/Homo_mermanus, /wiki/New_York_City_Pol...</td>\n",
       "      <td>, Joe Simon</td>\n",
       "      <td>, /wiki/Joe_Simon</td>\n",
       "      <td>, William Blake Everett</td>\n",
       "      <td>, /wiki/William_Blake_Everett</td>\n",
       "      <td>May, 1940</td>\n",
       "      <td>Prince Namor, the Sub-Mariner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Spider-Man:_The_Complete_Clone_Saga_Epic...</td>\n",
       "      <td>, /wiki/Peter_Parker_(Earth-616), /wiki/Ben_Re...</td>\n",
       "      <td>, /wiki/Kaine_Parker_(Earth-616), /wiki/Samuel...</td>\n",
       "      <td>, /wiki/Guardian_(Spider-Clone)_(Earth-616), /...</td>\n",
       "      <td>, Joe Quesada</td>\n",
       "      <td>, /wiki/Joe_Quesada</td>\n",
       "      <td>, J.M. DeMatteis</td>\n",
       "      <td>, /wiki/J.M._DeMatteis</td>\n",
       "      <td>1979</td>\n",
       "      <td>Resurrection!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Spider-Man:_The_Complete_Clone_Saga_Epic...</td>\n",
       "      <td>, /wiki/Peter_Parker_(Earth-616), /wiki/Ben_Re...</td>\n",
       "      <td>, /wiki/Miles_Warren_(Jackal_Clone_2)_(Earth-616)</td>\n",
       "      <td>, /wiki/Kaine_Parker_(Earth-616), /wiki/Charle...</td>\n",
       "      <td>, Joe Quesada</td>\n",
       "      <td>, /wiki/Joe_Quesada</td>\n",
       "      <td>, Howard Mackie</td>\n",
       "      <td>, /wiki/Howard_Mackie</td>\n",
       "      <td>1979</td>\n",
       "      <td>Truths &amp; Deceptions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Hellraiser_Vol_1_17</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Clive Barker</td>\n",
       "      <td>, /wiki/Clive_Barker</td>\n",
       "      <td>1992</td>\n",
       "      <td>Resurrection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Ultimate_Spider-Man_Infinite_Comic_Vol_2_10</td>\n",
       "      <td>, /wiki/Peter_Parker_(Earth-12041), /wiki/Pete...</td>\n",
       "      <td>, /wiki/Shazana_(Earth-12041)</td>\n",
       "      <td>, /wiki/William_Howard_Taft_(Earth-12041), /wi...</td>\n",
       "      <td>, Axel Alonso</td>\n",
       "      <td>, /wiki/Axel_Alonso</td>\n",
       "      <td>, John Barber</td>\n",
       "      <td>, /wiki/John_Barber</td>\n",
       "      <td>2016</td>\n",
       "      <td>Ham-ilton (Part 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Marvel_Universe_Ultimate_Spider-Man_vs._...</td>\n",
       "      <td>, /wiki/Web-Warriors_(Earth-12041), /wiki/Pete...</td>\n",
       "      <td>, /wiki/Norman_Osborn_(Earth-TRN457), /wiki/Hy...</td>\n",
       "      <td>, /wiki/Rio_Morales_(Earth-TRN457), /wiki/Jeff...</td>\n",
       "      <td>, Axel Alonso</td>\n",
       "      <td>, /wiki/Axel_Alonso</td>\n",
       "      <td>, Joe Caramagna</td>\n",
       "      <td>, /wiki/Joe_Caramagna</td>\n",
       "      <td>2016</td>\n",
       "      <td>st stor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68482 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL  \\\n",
       "0                /wiki/Marvel_Mystery_Comics_Vol_1_NN   \n",
       "0                        /wiki/Comedy_Comics_Vol_1_12   \n",
       "0                 /wiki/Marvel_Mystery_Comics_Vol_1_7   \n",
       "0                 /wiki/Marvel_Mystery_Comics_Vol_1_7   \n",
       "0                 /wiki/Marvel_Mystery_Comics_Vol_1_7   \n",
       "..                                                ...   \n",
       "0   /wiki/Spider-Man:_The_Complete_Clone_Saga_Epic...   \n",
       "0   /wiki/Spider-Man:_The_Complete_Clone_Saga_Epic...   \n",
       "0                           /wiki/Hellraiser_Vol_1_17   \n",
       "0   /wiki/Ultimate_Spider-Man_Infinite_Comic_Vol_2_10   \n",
       "0   /wiki/Marvel_Universe_Ultimate_Spider-Man_vs._...   \n",
       "\n",
       "                                      Good characters  \\\n",
       "0                                                       \n",
       "0                                                       \n",
       "0   , /wiki/Human_Torch_(Android)_(Earth-616), /wi...   \n",
       "0   , /wiki/Thomas_Halloway_(Earth-616), /wiki/Bet...   \n",
       "0   , /wiki/Namor_McKenzie_(Earth-616), /wiki/Thak...   \n",
       "..                                                ...   \n",
       "0   , /wiki/Peter_Parker_(Earth-616), /wiki/Ben_Re...   \n",
       "0   , /wiki/Peter_Parker_(Earth-616), /wiki/Ben_Re...   \n",
       "0                                                       \n",
       "0   , /wiki/Peter_Parker_(Earth-12041), /wiki/Pete...   \n",
       "0   , /wiki/Web-Warriors_(Earth-12041), /wiki/Pete...   \n",
       "\n",
       "                                       Bad characters  \\\n",
       "0                                                       \n",
       "0                                                       \n",
       "0   , /wiki/Roglo_(Earth-616), #cite_note-Only_App...   \n",
       "0                     , /wiki/Emma_Martin_(Earth-616)   \n",
       "0                                                       \n",
       "..                                                ...   \n",
       "0   , /wiki/Kaine_Parker_(Earth-616), /wiki/Samuel...   \n",
       "0   , /wiki/Miles_Warren_(Jackal_Clone_2)_(Earth-616)   \n",
       "0                                                       \n",
       "0                       , /wiki/Shazana_(Earth-12041)   \n",
       "0   , /wiki/Norman_Osborn_(Earth-TRN457), /wiki/Hy...   \n",
       "\n",
       "                                   Neutral characters Editor-in-chief  \\\n",
       "0                                                                       \n",
       "0                                                          , Stan Lee   \n",
       "0   , /wiki/New_York_City_Police_Department_(Earth...     , Joe Simon   \n",
       "0                    , /wiki/Henry_Martin_(Earth-616)     , Joe Simon   \n",
       "0   , /wiki/Homo_mermanus, /wiki/New_York_City_Pol...     , Joe Simon   \n",
       "..                                                ...             ...   \n",
       "0   , /wiki/Guardian_(Spider-Clone)_(Earth-616), /...   , Joe Quesada   \n",
       "0   , /wiki/Kaine_Parker_(Earth-616), /wiki/Charle...   , Joe Quesada   \n",
       "0                                                                       \n",
       "0   , /wiki/William_Howard_Taft_(Earth-12041), /wi...   , Axel Alonso   \n",
       "0   , /wiki/Rio_Morales_(Earth-TRN457), /wiki/Jeff...   , Axel Alonso   \n",
       "\n",
       "    Editor-in-chief URL                      Writer  \\\n",
       "0                                   , Joe Caramagna   \n",
       "0      , /wiki/Stan_Lee                               \n",
       "0     , /wiki/Joe_Simon    , Stan Lee, Larry Lieber   \n",
       "0     , /wiki/Joe_Simon  , Paul Gustavson, Ray Gill   \n",
       "0     , /wiki/Joe_Simon     , William Blake Everett   \n",
       "..                  ...                         ...   \n",
       "0   , /wiki/Joe_Quesada            , J.M. DeMatteis   \n",
       "0   , /wiki/Joe_Quesada             , Howard Mackie   \n",
       "0                                    , Clive Barker   \n",
       "0   , /wiki/Axel_Alonso               , John Barber   \n",
       "0   , /wiki/Axel_Alonso             , Joe Caramagna   \n",
       "\n",
       "                                Writer URL Publication date  \\\n",
       "0                    , /wiki/Joe_Caramagna    January, 1943   \n",
       "0                                            December, 1942   \n",
       "0     , /wiki/Stan_Lee, /wiki/Larry_Lieber        May, 1940   \n",
       "0   , /wiki/Paul_Gustavson, /wiki/Ray_Gill        May, 1940   \n",
       "0            , /wiki/William_Blake_Everett        May, 1940   \n",
       "..                                     ...              ...   \n",
       "0                   , /wiki/J.M._DeMatteis             1979   \n",
       "0                    , /wiki/Howard_Mackie             1979   \n",
       "0                     , /wiki/Clive_Barker             1992   \n",
       "0                      , /wiki/John_Barber             2016   \n",
       "0                    , /wiki/Joe_Caramagna             2016   \n",
       "\n",
       "                         Subcomic  \n",
       "0                         st stor  \n",
       "0                          Morphy  \n",
       "0                 The Human Torch  \n",
       "0        The Angel: Master of Men  \n",
       "0   Prince Namor, the Sub-Mariner  \n",
       "..                            ...  \n",
       "0                   Resurrection!  \n",
       "0             Truths & Deceptions  \n",
       "0                    Resurrection  \n",
       "0              Ham-ilton (Part 2)  \n",
       "0                         st stor  \n",
       "\n",
       "[68482 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data_pickle/comics_marvel.txt', 'rb') as f:\n",
    "    comics_marvel = pickle.load(f)\n",
    "\n",
    "comics_marvel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: DC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's note that the DC dataset is extremely similar to the Marvel dataset. Indeed, the characters have the same characteristics and we can thus use the same attributes. The difference is that the characters are in two different pages which are classified into good and bad characters (compared to the Marvel dataset where we took all the characters in `Earth-616`, not knowing if they were good or bad). Moreover, the comics are also very similarly written and we can apply a similar code as the one we did for Marvel. We just have to be careful when we use the different `find` functions. Indeed, even if the characteristics are the same, the html code might be different and we had to adapt the code we had.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Parse the DC characters:\n",
    "\n",
    "### 2.1.1 Good DC characters\n",
    "\n",
    "**Collect the first URLs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://dc.fandom.com/wiki/Category:Good_Characters'\n",
    "r = requests.get(URL)\n",
    "page_body = r.text\n",
    "soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "# Collect URL of each character in the web page\n",
    "publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "\n",
    "list_url = []\n",
    "for p in publications_wrappers:\n",
    "    for a in p.find_all('a', href=True):\n",
    "        list_url.append(a['href'])\n",
    "        \n",
    "# Remove duplicate\n",
    "my_set = set(list_url)\n",
    "good_list_url = list(my_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the first pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['URL', 'Real Name', 'Identity', 'Current Alias', 'Citizenship',\n",
    "                'Marital Status', 'Occupation',\n",
    "                'Education', 'Gender', 'Height', 'Weight',\n",
    "                'Eyes','Hair',\n",
    "                'Place of Birth']\n",
    "personnage_pd = pd.DataFrame(columns=new_columns)\n",
    "idx = 0\n",
    "dict_geant = {}\n",
    "for pers in good_list_url:\n",
    "    # Get URL and use html parser\n",
    "    URL_char = 'https://dc.fandom.com/' + pers\n",
    "    URL_char = URL_char.replace(\"'\",\"\")\n",
    "    r_char = requests.get(URL_char)\n",
    "    page_body_char = r_char.text\n",
    "    soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "    \n",
    "    # Initialize variables\n",
    "    url, name, identity, current_alias, citizenship, marital_status, occupation = '','','','','','',''\n",
    "    education, gender, height, weight, eyes, hair, place_of_birth = '','','','','','',''\n",
    "    \n",
    "    personnage =[]\n",
    "    \n",
    "    # Parsing\n",
    "    url = URL_char\n",
    "    side_tab = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "    for p in side_tab:\n",
    "        for pp in p.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
    "            for ppp in pp.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "                if(ppp.text[0:]==\"Real Name\"):\n",
    "                    name = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Identity\"):\n",
    "                    identity = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Current Alias\"):\n",
    "                    current_alias = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Citizenship\"):\n",
    "                    citizenship = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Marital Status\"):\n",
    "                    marital_status = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Occupation\"):\n",
    "                    occupation = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Education\"):\n",
    "                    education = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Gender\"):\n",
    "                    gender = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Height\"):\n",
    "                    height = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Weight\"):\n",
    "                    weight = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Eyes\"):\n",
    "                    eyes = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Hair\"):\n",
    "                    hair = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Place of Birth\"):\n",
    "                    place_of_birth = pp.find('div', class_='pi-data-value pi-font').text\n",
    "\n",
    "        characteristics_pd = pd.DataFrame([[url, name[0:], identity, current_alias, citizenship, marital_status, occupation,\n",
    "                          education, gender, height, weight, eyes, hair, place_of_birth]], columns = new_columns)\n",
    "\n",
    "        personnage_pd = personnage_pd.append(characteristics_pd,ignore_index=True)\n",
    "    \n",
    "nextpage = soup.find('link', {\"rel\" : \"next\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the remaining pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(len(nextpage['href'])):\n",
    "    urlnext_page = nextpage['href']\n",
    "    r = requests.get(urlnext_page)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "    # Collect URL of each character in the web page\n",
    "    publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "    list_url = []\n",
    "    for p in publications_wrappers:\n",
    "        for a in p.find_all('a', href=True):\n",
    "            list_url.append(a['href'])\n",
    "            \n",
    "    # Remove duplicate\n",
    "    my_set =set(list_url)\n",
    "    good_list_url = list(my_set)\n",
    "    idx =0\n",
    "    dict_geant={}\n",
    "    for pers in good_list_url:\n",
    "        # Get URL and use html parser\n",
    "        URL_char = 'https://dc.fandom.com/' + pers\n",
    "        URL_char = URL_char.replace(\"'\",\"\")\n",
    "        r_char = requests.get(URL_char)\n",
    "        page_body_char = r_char.text\n",
    "        soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "        \n",
    "    # Initialize variables\n",
    "    url, name, identity, current_alias, citizenship, marital_status, occupation = '','','','','','',''\n",
    "    education, gender, height, weight, eyes, hair, place_of_birth = '','','','','','',''\n",
    "    \n",
    "    personnage =[]\n",
    "    \n",
    "    # Parsing\n",
    "    url = URL_char\n",
    "    side_tab = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "    for p in side_tab:\n",
    "        for pp in p.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
    "            for ppp in pp.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "            if(ppp.text[0:]==\"Real Name\"):\n",
    "                name = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Identity\"):\n",
    "                identity = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Current Alias\"):\n",
    "                current_alias = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Citizenship\"):\n",
    "                citizenship = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Marital Status\"):\n",
    "                marital_status = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Occupation\"):\n",
    "                occupation = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Education\"):\n",
    "                education = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Gender\"):\n",
    "                gender = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Height\"):\n",
    "                height = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Weight\"):\n",
    "                weight = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Eyes\"):\n",
    "                eyes = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Hair\"):\n",
    "                hair = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Place of Birth\"):\n",
    "                place_of_birth = pp.find('div', class_='pi-data-value pi-font').text\n",
    "\n",
    "    characteristics_pd = pd.DataFrame([[url, name[0:], identity, current_alias, citizenship, marital_status, occupation,\n",
    "                      education, gender, height, weight, eyes, hair, place_of_birth]], columns = new_columns)\n",
    "\n",
    "    personnage_pd = personnage_pd.append(characteristics_pd,ignore_index=True)\n",
    "    \n",
    "    nextpage = soup.find('link', {\"rel\" : \"next\"})\n",
    "    \n",
    "# Create a column to classify the characters as good ones\n",
    "personnage_pd.insert(5,'Good or Bad', \"Good\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Bad DC characters\n",
    "\n",
    "**Collect the first URLs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://dc.fandom.com/wiki/Category:Bad_Characters'\n",
    "r = requests.get(URL)\n",
    "page_body = r.text\n",
    "soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "# Collect URL of each character in the web page\n",
    "publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "\n",
    "list_url = []\n",
    "for p in publications_wrappers:\n",
    "    for a in p.find_all('a', href=True):\n",
    "        list_url.append(a['href'])\n",
    "        \n",
    "# Remove duplicate\n",
    "my_set = set(list_url)\n",
    "good_list_url = list(my_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the first pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['URL', 'Real Name', 'Identity', 'Current Alias', 'Citizenship',\n",
    "                'Marital Status', 'Occupation',\n",
    "                'Education', 'Gender', 'Height', 'Weight',\n",
    "                'Eyes','Hair',\n",
    "                'Place of Birth']\n",
    "bad_pers_dc = pd.DataFrame(columns=new_columns)\n",
    "idx = 0\n",
    "dict_geant = {}\n",
    "for pers in good_list_url:\n",
    "    # Get URL and use html parser\n",
    "    URL_char = 'https://dc.fandom.com/' + pers\n",
    "    URL_char = URL_char.replace(\"'\",\"\")\n",
    "    r_char = requests.get(URL_char)\n",
    "    page_body_char = r_char.text\n",
    "    soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "    \n",
    "    # Initialize variables\n",
    "    url, name, identity, current_alias, citizenship, marital_status, occupation = '','','','','','',''\n",
    "    education, gender, height, weight, eyes, hair, place_of_birth = '','','','','','',''\n",
    "    \n",
    "    personnage =[]\n",
    "    \n",
    "    # Parsing\n",
    "    url = URL_char\n",
    "    side_tab = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "    for p in side_tab:\n",
    "        for pp in p.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
    "            for ppp in pp.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "                if(ppp.text[0:]==\"Real Name\"):\n",
    "                    name = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Identity\"):\n",
    "                    identity = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Current Alias\"):\n",
    "                    current_alias = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Citizenship\"):\n",
    "                    citizenship = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Marital Status\"):\n",
    "                    marital_status = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Occupation\"):\n",
    "                    occupation = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Education\"):\n",
    "                    education = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Gender\"):\n",
    "                    gender = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Height\"):\n",
    "                    height = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Weight\"):\n",
    "                    weight = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Eyes\"):\n",
    "                    eyes = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Hair\"):\n",
    "                    hair = pp.find('div', class_='pi-data-value pi-font').text\n",
    "                if(ppp.text[0:]==\"Place of Birth\"):\n",
    "                    place_of_birth = pp.find('div', class_='pi-data-value pi-font').text\n",
    "\n",
    "        characteristics_pd = pd.DataFrame([[url, name[0:], identity, current_alias, citizenship, marital_status, occupation,\n",
    "                          education, gender, height, weight, eyes, hair, place_of_birth]], columns = new_columns)\n",
    "\n",
    "        bad_pers_dc = bad_pers_dc.append(characteristics_pd,ignore_index=True)\n",
    "    \n",
    "nextpage = soup.find('link', {\"rel\" : \"next\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the remaining pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(len(nextpage['href'])):\n",
    "    urlnext_page = nextpage['href']\n",
    "    r = requests.get(urlnext_page)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "    # Collect URL of each character in the web page\n",
    "    publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "    list_url = []\n",
    "    for p in publications_wrappers:\n",
    "        for a in p.find_all('a', href=True):\n",
    "            list_url.append(a['href'])\n",
    "            \n",
    "    # Remove duplicate\n",
    "    my_set =set(list_url)\n",
    "    good_list_url = list(my_set)\n",
    "    idx =0\n",
    "    dict_geant={}\n",
    "    for pers in good_list_url:\n",
    "        # Get URL and use html parser\n",
    "        URL_char = 'https://dc.fandom.com/' + pers\n",
    "        URL_char = URL_char.replace(\"'\",\"\")\n",
    "        r_char = requests.get(URL_char)\n",
    "        page_body_char = r_char.text\n",
    "        soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "        \n",
    "    # Initialize variables\n",
    "    url, name, identity, current_alias, citizenship, marital_status, occupation = '','','','','','',''\n",
    "    education, gender, height, weight, eyes, hair, place_of_birth = '','','','','','',''\n",
    "    \n",
    "    personnage =[]\n",
    "    \n",
    "    # Parsing\n",
    "    url = URL_char\n",
    "    side_tab = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "    for p in side_tab:\n",
    "        for pp in p.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
    "            for ppp in pp.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "            if(ppp.text[0:]==\"Real Name\"):\n",
    "                name = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Identity\"):\n",
    "                identity = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Current Alias\"):\n",
    "                current_alias = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Citizenship\"):\n",
    "                citizenship = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Marital Status\"):\n",
    "                marital_status = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Occupation\"):\n",
    "                occupation = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Education\"):\n",
    "                education = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Gender\"):\n",
    "                gender = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Height\"):\n",
    "                height = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Weight\"):\n",
    "                weight = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Eyes\"):\n",
    "                eyes = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Hair\"):\n",
    "                hair = pp.find('div', class_='pi-data-value pi-font').text\n",
    "            if(ppp.text[0:]==\"Place of Birth\"):\n",
    "                place_of_birth = pp.find('div', class_='pi-data-value pi-font').text\n",
    "\n",
    "    characteristics_pd = pd.DataFrame([[url, name[0:], identity, current_alias, citizenship, marital_status, occupation,\n",
    "                      education, gender, height, weight, eyes, hair, place_of_birth]], columns = new_columns)\n",
    "\n",
    "    bad_pers_dc = bad_pers_dc.append(characteristics_pd,ignore_index=True)\n",
    "    \n",
    "    nextpage = soup.find('link', {\"rel\" : \"next\"})\n",
    "    \n",
    "# Create a column to classify the characters as bad ones\n",
    "personnage_pd.insert(5,'Good or Bad', \"Bad\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Join dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenate the dataframes for good and bad characters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_dc = [personnage_pd, bad_pers_dc]\n",
    "pd.concat(pers_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save to pickle:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pers_dc, open('data_pickle/perso_dc.txt','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open the saved file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_dc = pd.read_pickle('data_pickle/perso_dc.txt')\n",
    "\n",
    "character_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Parse the DC comics:\n",
    "\n",
    "**Collect the first URLs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get URL and use html parser\n",
    "URL = 'https://dc.fandom.com/wiki/Category:Comics'\n",
    "r = requests.get(URL)\n",
    "page_body = r.text\n",
    "soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "# Collect URL of each character in the web page\n",
    "publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "\n",
    "# Initialization\n",
    "i = 0\n",
    "list_url = []\n",
    "\n",
    "for p in publications_wrappers:\n",
    "    # Don't take into account the first links which are \"Categories\"\n",
    "    if(i>=24):\n",
    "        for a in p.find_all('a', href=True):\n",
    "            list_url.append(a['href'])\n",
    "    i +=1\n",
    "    \n",
    "# Remove duplicate\n",
    "my_set = set(list_url)\n",
    "good_list_url = list(my_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the first pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comics_columns = ['URL', 'Good characters', 'Bad characters', \n",
    "                  'Neutral characters', 'Editor-in-chief', 'Editor-in-chief URL', \n",
    "                  'Writer', 'Writer URL', 'Publication date', 'Subcomic']\n",
    "\n",
    "comics_pd = pd.DataFrame(columns=comics_columns)\n",
    "\n",
    "# Parse the first page\n",
    "for comics in good_list_url:\n",
    "    # Get URL and use html parser\n",
    "    URL_char = 'https://dc.fandom.com' + comics\n",
    "    URL_char = URL_char.replace(\"'\",\"\")\n",
    "    r_char = requests.get(URL_char)\n",
    "    page_body_char = r_char.text\n",
    "    soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "    \n",
    "    # Initialization\n",
    "    good, bad, neutral = '','',''\n",
    "    editor, writer, publication = '','',''\n",
    "    editorURL, writerURL, subcomic = '','',''\n",
    "    URL2 = URL_char.replace('https://dc.fandom.com','')\n",
    "    \n",
    "    # Boolean for the first case\n",
    "    first = 1\n",
    "    \n",
    "    # Parse characters appearances\n",
    "    appearances = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "    for p in appearances:\n",
    "        for pp in p.find_all('p'):\n",
    "            # If a comic is split in sub-comics\n",
    "            span = pp.find_previous('span', class_='mw-headline')\n",
    "            if span and (\"Appearing\" in span.text):\n",
    "                if (not first and subcomic!=span.text \\\n",
    "                and (good or bad or neutral)):\n",
    "                    # Take only the title of the subcomic\n",
    "                    subcomic = subcomic[14:-1]\n",
    "                    appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                        editor, editorURL,\n",
    "                                        writer, writerURL,\n",
    "                                        publication, subcomic]], columns = comics_columns)\n",
    "                    comics_pd = comics_pd.append(appearances_pd)\n",
    "                    good, bad, neutral = '','',''\n",
    "                    editor, writer, publication = '','',''\n",
    "                    editorURL, writerURL = '',''\n",
    "                    subcomic = span.text\n",
    "                else:\n",
    "                    first = 0\n",
    "                    subcomic = span.text\n",
    "                    \n",
    "            if \"Featured Characters:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    good = good + ', ' + a['href']\n",
    "            if \"Supporting Characters:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    good = good + ', ' + a['href']\n",
    "            if \"Antagonists:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    bad = bad + ', ' + a['href']\n",
    "            if \"Other Characters:\" in pp.text:\n",
    "                ul = pp.find_next('ul')\n",
    "                for a in ul.find_all('a', href=True):\n",
    "                    neutral = neutral + ', ' + a['href']\n",
    "                    \n",
    "    # Each subcomic is written in the format '\"Appearing in *subcomic*\"'\n",
    "    # We keep only the subcomic name\n",
    "    subcomic = subcomic[14:-1]\n",
    "    appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                    editor, editorURL,\n",
    "                                    writer, writerURL,\n",
    "                                    publication, subcomic]], columns = comics_columns)\n",
    "    comics_pd = comics_pd.append(appearances_pd)\n",
    "    subcomic = ''\n",
    "    \n",
    "    \n",
    "    # Parse comic characteristics\n",
    "    side_tab = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "    for p in side_tab:\n",
    "        # Publication date\n",
    "        if(p.find_next('h2', {'data-source':'StoryTitle1'})):\n",
    "            tab = p.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "        if(tab):\n",
    "            tab = tab.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "        if(tab):\n",
    "            tab = tab.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "        if(tab):\n",
    "            publication = tab.text\n",
    "            \n",
    "        # Editors\n",
    "        for pp in p.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "            if ('Executive Editor' in pp.text):\n",
    "                ppp = pp.find_next('div', class_='pi-data-value pi-font')\n",
    "                for a in ppp.find_all('a'):\n",
    "                    editor = editor + ', ' + a.text\n",
    "                for a in ppp.find_all('a', href=True):\n",
    "                    editorURL = editorURL + ', ' + a['href']\n",
    "\n",
    "        sub = tab.find_next('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "\n",
    "        # Writers\n",
    "        for pp in p.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "            if(pp.find_previous('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')):\n",
    "                sub = pp.find_previous('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "            if sub:\n",
    "                adiv = sub.find_next('h3', class_='pi-data-label pi-secondary-font')\n",
    "            if (adiv and 'Writer' in adiv.text and 'Writer' in pp.text):\n",
    "                if (sub):\n",
    "                    subcomic = sub.text[1:-1]\n",
    "                ppp = pp.find_next('div', class_='pi-data-value pi-font')\n",
    "                for a in ppp.find_all('a'):\n",
    "                    writer = writer + ', ' + a.text\n",
    "                for a in ppp.find_all('a', href=True):\n",
    "                    writerURL = writerURL + ', ' + a['href']\n",
    "                comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer'] = writer\n",
    "                comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer URL'] = writerURL\n",
    "                if (sub):\n",
    "                    sub = sub.find_next('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "                    writer, writerURL = '',''\n",
    "            elif (sub): \n",
    "                sub = sub.find_next('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "                writer, writerURL = '',''    \n",
    "                \n",
    "    comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief'] = editor\n",
    "    comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief URL'] = editorURL\n",
    "    comics_pd.loc[comics_pd.URL == URL2,'Publication date'] = publication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the remaining pages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all the other pages\n",
    "nextpage = soup.find('link', {\"rel\" : \"next\"})\n",
    "\n",
    "i = 0\n",
    "tot_page = 49759/200\n",
    "\n",
    "while(len(nextpage['href'])):\n",
    "    # Print a loading bar\n",
    "    i += 1\n",
    "    printed= i/tot_page*100\n",
    "    stdout.write(\"\\r%f %%\" % printed)\n",
    "    stdout.flush()\n",
    "    urlnext_page = nextpage['href']\n",
    "    r = requests.get(urlnext_page)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    \n",
    "    # Collect url of each character in the web page\n",
    "    publications_wrappers = soup.find_all('li', class_='category-page__member')\n",
    "    list_url = []\n",
    "    for p in publications_wrappers:\n",
    "        for a in p.find_all('a', href=True):\n",
    "            list_url.append(a['href'])\n",
    "          \n",
    "    # Remove duplicates\n",
    "    my_set = set(list_url)\n",
    "    good_list_url = list(my_set)\n",
    "    good_list_url = [ x for x in good_list_url if \"/wiki/Category:\" not in x ]\n",
    "  \n",
    "    # Parse the first page\n",
    "    for comics in good_list_url:\n",
    "        # Get URL and use html parser\n",
    "        URL_char = 'https://dc.fandom.com' + comics\n",
    "        URL_char = URL_char.replace(\"'\",\"\")\n",
    "        r_char = requests.get(URL_char)\n",
    "        page_body_char = r_char.text\n",
    "        soup_char = BeautifulSoup(page_body_char, 'html.parser')\n",
    "        \n",
    "        # Initialization\n",
    "        good, bad, neutral = '','',''\n",
    "        editor, writer, publication = '','',''\n",
    "        editorURL, writerURL, subcomic = '','',''\n",
    "        URL2 = URL_char.replace('https://dc.fandom.com','')\n",
    "\n",
    "        # Boolean for the first case\n",
    "        first = 1\n",
    "\n",
    "        # Parse characters appearances\n",
    "        appearances = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "        for p in appearances:\n",
    "            for pp in p.find_all('p'):\n",
    "                # If a comic is split in sub-comics\n",
    "                span = pp.find_previous('span', class_='mw-headline')\n",
    "                if span and (\"Appearing\" in span.text):\n",
    "                    if (not first and subcomic!=span.text \\\n",
    "                    and (good or bad or neutral)):\n",
    "                        # Take only the title of the subcomic\n",
    "                        subcomic = subcomic[14:-1]\n",
    "                        appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                              editor, editorURL,\n",
    "                                              writer, writerURL,\n",
    "                                              publication, subcomic]], columns = comics_columns)\n",
    "                        comics_pd = comics_pd.append(appearances_pd)\n",
    "\n",
    "                        good, bad, neutral = '','',''\n",
    "                        editor, writer, publication = '','',''\n",
    "                        editorURL, writerURL = '',''\n",
    "                        subcomic = span.text\n",
    "                    else:\n",
    "                        first = 0\n",
    "                        subcomic = span.text\n",
    "                if \"Featured Characters:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        good = good + ', ' + a['href']\n",
    "                if \"Supporting Characters:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        good = good + ', ' + a['href']\n",
    "                if \"Antagonists:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        bad = bad + ', ' + a['href']\n",
    "                if \"Other Characters:\" in pp.text:\n",
    "                    ul = pp.find_next('ul')\n",
    "                    for a in ul.find_all('a', href=True):\n",
    "                        neutral = neutral + ', ' + a['href']\n",
    "\n",
    "        # Each subcomic is written in the format '\"Appearing in *subcomic*\"'\n",
    "        # We keep only the subcomic name\n",
    "        subcomic = subcomic[14:-1]\n",
    "\n",
    "        appearances_pd = pd.DataFrame([[URL2, good, bad, neutral, \n",
    "                                          editor, editorURL,\n",
    "                                          writer, writerURL,\n",
    "                                          publication, subcomic]], columns = comics_columns)\n",
    "\n",
    "        comics_pd = comics_pd.append(appearances_pd)\n",
    "\n",
    "        subcomic = ''\n",
    "\n",
    "\n",
    "        # Parse comic characteristics\n",
    "        side_tab = soup_char.find_all('div', class_='mw-content-ltr mw-content-text')\n",
    "        for p in side_tab:\n",
    "            # Publication date\n",
    "            if(p.find_next('h2', {'data-source':'StoryTitle1'})):\n",
    "                tab = p.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "                if(tab):\n",
    "                    tab = tab.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "                if(tab):\n",
    "                    tab = tab.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "                if(tab):\n",
    "                    publication = tab.text\n",
    "            else:\n",
    "                if(tab):\n",
    "                    tab = p.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "                if(tab):\n",
    "                    tab = tab.find_next('h2', class_='pi-item pi-item-spacing pi-title')\n",
    "                if(tab):\n",
    "                    publication = tab.text\n",
    "\n",
    "            # Editors\n",
    "            for pp in p.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "                if ('Executive Editor' in pp.text):\n",
    "                    ppp = pp.find_next('div', class_='pi-data-value pi-font')\n",
    "                    for a in ppp.find_all('a'):\n",
    "                        editor = editor + ', ' + a.text\n",
    "                    for a in ppp.find_all('a', href=True):\n",
    "                        editorURL = editorURL + ', ' + a['href']\n",
    "            if(tab):           \n",
    "                if(tab.find_next('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')):\n",
    "                      sub = tab.find_next('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "\n",
    "            # Writers\n",
    "            for pp in p.find_all('h3', class_='pi-data-label pi-secondary-font'):\n",
    "                if(pp.find_previous('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')):\n",
    "                    sub = pp.find_previous('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "                if sub:\n",
    "                    adiv = sub.find_next('h3', class_='pi-data-label pi-secondary-font')\n",
    "                if (adiv and 'Writer' in adiv.text and 'Writer' in pp.text):\n",
    "                    if (sub):\n",
    "                        subcomic = sub.text[1:-1]\n",
    "                        ppp = pp.find_next('div', class_='pi-data-value pi-font')\n",
    "                    for a in ppp.find_all('a'):\n",
    "                        writer = writer + ', ' + a.text\n",
    "                    for a in ppp.find_all('a', href=True):\n",
    "                        writerURL = writerURL + ', ' + a['href']\n",
    "                    comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer'] = writer\n",
    "                    comics_pd.loc[comics_pd.Subcomic == subcomic,'Writer URL'] = writerURL\n",
    "                    if (sub):\n",
    "                        sub = sub.find_next('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "                        writer, writerURL = '',''\n",
    "                elif (sub): \n",
    "                    sub = sub.find_next('h2', class_='pi-item pi-header pi-secondary-font pi-item-spacing pi-secondary-background')\n",
    "                    writer, writerURL = '',''    \n",
    "                \n",
    "        comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief'] = editor\n",
    "        comics_pd.loc[comics_pd.URL == URL2,'Editor-in-chief URL'] = editorURL\n",
    "        comics_pd.loc[comics_pd.URL == URL2,'Publication date'] = publication\n",
    "    nextpage = soup.find('link', {\"rel\" : \"next\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save to pickle:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(comics_pd, open('data_pickle/characters_dc.txt','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open the saved file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Good characters</th>\n",
       "      <th>Bad characters</th>\n",
       "      <th>Neutral characters</th>\n",
       "      <th>Editor-in-chief</th>\n",
       "      <th>Editor-in-chief URL</th>\n",
       "      <th>Writer</th>\n",
       "      <th>Writer URL</th>\n",
       "      <th>Publication date</th>\n",
       "      <th>Subcomic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/100_Bullets_Vol_1_64</td>\n",
       "      <td>, /wiki/Jack_Daw_(100_Bullets), /wiki/Philip_G...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Karen Berger</td>\n",
       "      <td>, /wiki/Karen_Berger</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>November, 2005</td>\n",
       "      <td>The Dive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/100_Bullets_Vol_1_25</td>\n",
       "      <td>, /wiki/Augustus_Medici_(100_Bullets), /wiki/B...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Karen Berger</td>\n",
       "      <td>, /wiki/Karen_Berger</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>August, 2001</td>\n",
       "      <td>Red Prince Blues (Part III of III)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/2020_Visions_Vol_1_5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Karen Berger</td>\n",
       "      <td>, /wiki/Karen_Berger</td>\n",
       "      <td>, Ron Marz</td>\n",
       "      <td>, /wiki/Ron_Marz</td>\n",
       "      <td>September, 1997</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/100%25_True%3F_Vol_1_2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Jenette Kahn</td>\n",
       "      <td>, /wiki/Jenette_Kahn</td>\n",
       "      <td>, Ron Marz</td>\n",
       "      <td>, /wiki/Ron_Marz</td>\n",
       "      <td>December, 1997</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/100_Bullets_Vol_1_11</td>\n",
       "      <td>, /wiki/Philip_Graves_(100_Bullets)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Karen Berger</td>\n",
       "      <td>, /wiki/Karen_Berger</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>June, 2000</td>\n",
       "      <td>Heartbreak, Sunny Side Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Zatanna_Vol_2_1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Dan DiDio</td>\n",
       "      <td>, /wiki/Dan_DiDio</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>July, 2010</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Zero_Girl_Vol_1_4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>, Jim Lee</td>\n",
       "      <td>, /wiki/Jim_Lee</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>May, 2001</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Young_Romance_Vol_1_196</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>December, 1973</td>\n",
       "      <td>he 1st Stor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Young_Romance_Vol_1_126</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>November, 1963</td>\n",
       "      <td>he 1st Stor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/wiki/Young_Romance_Vol_1_200</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>August, 1974</td>\n",
       "      <td>he 1st Stor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62314 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              URL  \\\n",
       "0      /wiki/100_Bullets_Vol_1_64   \n",
       "0      /wiki/100_Bullets_Vol_1_25   \n",
       "0      /wiki/2020_Visions_Vol_1_5   \n",
       "0    /wiki/100%25_True%3F_Vol_1_2   \n",
       "0      /wiki/100_Bullets_Vol_1_11   \n",
       "..                            ...   \n",
       "0           /wiki/Zatanna_Vol_2_1   \n",
       "0         /wiki/Zero_Girl_Vol_1_4   \n",
       "0   /wiki/Young_Romance_Vol_1_196   \n",
       "0   /wiki/Young_Romance_Vol_1_126   \n",
       "0   /wiki/Young_Romance_Vol_1_200   \n",
       "\n",
       "                                      Good characters Bad characters  \\\n",
       "0   , /wiki/Jack_Daw_(100_Bullets), /wiki/Philip_G...                  \n",
       "0   , /wiki/Augustus_Medici_(100_Bullets), /wiki/B...                  \n",
       "0                                                                      \n",
       "0                                                                      \n",
       "0                 , /wiki/Philip_Graves_(100_Bullets)                  \n",
       "..                                                ...            ...   \n",
       "0                                                                      \n",
       "0                                                                      \n",
       "0                                                                      \n",
       "0                                                                      \n",
       "0                                                                      \n",
       "\n",
       "   Neutral characters Editor-in-chief   Editor-in-chief URL      Writer  \\\n",
       "0                      , Karen Berger  , /wiki/Karen_Berger               \n",
       "0                      , Karen Berger  , /wiki/Karen_Berger               \n",
       "0                      , Karen Berger  , /wiki/Karen_Berger  , Ron Marz   \n",
       "0                      , Jenette Kahn  , /wiki/Jenette_Kahn  , Ron Marz   \n",
       "0                      , Karen Berger  , /wiki/Karen_Berger               \n",
       "..                ...             ...                   ...         ...   \n",
       "0                         , Dan DiDio     , /wiki/Dan_DiDio               \n",
       "0                           , Jim Lee       , /wiki/Jim_Lee               \n",
       "0                                                                         \n",
       "0                                                                         \n",
       "0                                                                         \n",
       "\n",
       "          Writer URL Publication date                            Subcomic  \n",
       "0                      November, 2005                            The Dive  \n",
       "0                        August, 2001  Red Prince Blues (Part III of III)  \n",
       "0   , /wiki/Ron_Marz  September, 1997                                      \n",
       "0   , /wiki/Ron_Marz   December, 1997                                      \n",
       "0                          June, 2000           Heartbreak, Sunny Side Up  \n",
       "..               ...              ...                                 ...  \n",
       "0                          July, 2010                                      \n",
       "0                           May, 2001                                      \n",
       "0                      December, 1973                         he 1st Stor  \n",
       "0                      November, 1963                         he 1st Stor  \n",
       "0                        August, 1974                         he 1st Stor  \n",
       "\n",
       "[62314 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data_pickle/comics_dc.txt', 'rb') as f:\n",
    "    comics_pd = pickle.load(f)\n",
    "\n",
    "comics_pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
